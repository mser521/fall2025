---
title: "Bias"
date: "2025-08-26"
type: "activity"
due_date: "2025-09-11"
draft: 0
---

## 1. Safiya Noble Video
Video: Imagining a future free from the algorithms of oppression [Conference keynote]. Association for Computational Linguistics (ACL 2019), Florence, Italy. https://www.youtube.com/watch?v=tNi_U1Bb1S0&t=772s

1. How are patterns of oppression replicated or magnified by modern technologies? Give some examples.
1. How are the biases reproduced by search engines (and other platforms) similar and different from previous kinds of information intermediaries (libraries, family & friends, consulting with experts)?
1. For constituencies that don’t have the money, technological expertise, numbers, or influence to substantially influence search results: what recourse should they have? Shouldn’t people get a say in how their likenesses are being represented?
1. What obligations do organizations have to ensure that the data and narratives they produce about people do not reinforce allocative or representational harms?


## 2. Kate Crawford Video 
Video: “The Trouble with Bias” (2017) - Kate Crawford. https://www.youtube.com/watch?v=fMym_BKWQzk 

1. What distinguishes allocative harms (e.g., unequal access to loans, jobs) from representational harms (e.g., cultural stereotyping or erasure)? Can you think of a real-world example (either familiar or hypothetical) for each? 
    * How does one impact the other?
    * Which type of harm is understudied? Why is it understudied?

2. Crawford uses the example of historical redlining and modern delivery disparities to illustrate how past biases are encoded into current systems. Consider the following context (below). How might historical inequities impact the fairness of these systems:
    * Predictive policing
    * Credit scores
    * Facial recognition
    * Automated hiring tools
    * Education placement  (should Susie be in the gifted program?)

3. Crawford reminds us that every design choice carries social consequences. Who typically gets to make decisions about algorithms and datasets, and who feels their consequences most? How can we democratize these decisions?



## 3. Abigail Thorn Video
Video: Thorn, Abigail (2021, July). Social Constructs (YouTube Video). Philosophy Tube. https://www.youtube.com/watch?v=koud7hgGyQ8

1. Thorne pulls out a quote from Ásta (Icelandic philosopher), that considers whether a property of a person is “socially significant.”
    * What is the difference between a "natural property" and a "social construct"?
    * What example does Thorne gives to demonstrate this idea? Can you think of additional examples?

1. Around minute 7:30, Thorne says: 

    > “What’s all this stuff about ‘natural properties?’ The only reason that the people of Earth 2 bother to measure height...the only reason they have a concept of height...is that they care about Schmeight. The act of measuring someone’s height is not a neutral thing that happens before they get assigned “big” or “mini. When you measure something, you’re already assuming that there is something there worth measuring. So these supposedly objective, natural properties of yours are also social constructs. It’s not augmented reality. It’s full VR, son!”

    Question: are all databases “full VR”? Is there such thing as an objective property?

1. Do we measure things because they’re important, or do things become important because we measure them? What example does Thorne give re: “Earth 2” + “Earth 0”?

1. Why are these systems so difficult to change?