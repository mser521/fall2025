---
title: "Bias"
date: "2025-08-26"
type: "activity"
due_date: "2025-09-11"
draft: 0
---

## Kate Crawford Video 
Video: “The Trouble with Bias” (2017) - Kate Crawford. https://www.youtube.com/watch?v=fMym_BKWQzk 

1. What distinguishes allocative harms (e.g., unequal access to loans, jobs) from representational harms (e.g., cultural stereotyping or erasure)? Can you think of a real-world example (either familiar or hypothetical) for each? 
    * How does one impact the other?
    * Which type of harm is understudied? Why is it understudied?

2. You are working at a company that is scaling up their software platform to new cities. You're collaborating with a data scientist who has been careful to take a representative sample of people who are using their software, in order to create a data-informed plan to scale up. Have they done enough to mitigate bias? Why or why not?

3. Crawford uses the example of historical redlining and modern delivery disparities to illustrate how past biases are encoded into current systems. Can you identify other contexts where historical inequities continue to impact contemporary algorithmic systems?

4. She reminds us that every design choice carries social consequences. Who typically gets to make decisions about algorithms and datasets, and who feels their consequences most? How can we democratize these decisions?

## Safiya Noble Video
Video: Imagining a future free from the algorithms of oppression [Conference keynote]. Association for Computational Linguistics (ACL 2019), Florence, Italy. https://www.youtube.com/watch?v=tNi_U1Bb1S0&t=772s"

1. How are patterns of oppression replicated or magnified by modern technologies?
1. Can you identify examples where marginalized communities continue to be disproportionately targeted or misrepresented by algorithmic systems?
1. What obligations do organizations have to ensure that the data and narratives they produce about people do not reinforce allocative or representational harms?


## Abigail Thorn Video
Video: Thorn, Abigail (2021, July). Social Constructs (YouTube Video). Philosophy Tube. https://www.youtube.com/watch?v=koud7hgGyQ8

1. What is a social construct? 

1. Thorne argues that classification systems (like gender categories) are socially constructed and historically situated. Why does it matter what categories we choose, and what are the consequences of using outdated or arbitrary classifications in our databases and data-informed decisionmaking?

1. Around minute 7:30, Thorne says: 

    > “What’s all this stuff about ‘natural properties?’ The only reason that the people of Earth 2 bother to measure height...the only reason they have a concept of height...is that they care about Schmeight. The act of measuring someone’s height is not a neutral thing that happens before they get assigned “big” or “mini. When you measure something, you’re already assuming that there is something there worth measuring. So these supposedly objective, natural properties of yours are also social constructs. It’s not augmented reality. It’s full VR, son!”

    Question: are all databases “full VR”? Is there such thing as an objective property?

1. Why do categorization and classification systems shape behavior? 

1. Why are these systems so difficult to change?